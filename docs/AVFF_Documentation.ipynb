{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AVFF (Audio-Visual Feature Fusion) Documentation\n",
    "\n",
    "This documentation provides a comprehensive guide to the AVFF deepfake detection system, including explanations, code examples, and visualizations.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Preprocessing](#preprocessing)\n",
    "   - [Image Preprocessing](#image-preprocessing)\n",
    "   - [Audio Preprocessing](#audio-preprocessing)\n",
    "     - [Mel Spectrograms](#mel-spectrograms)\n",
    "2. [Model Architecture](#model-architecture)\n",
    "   - [Audio Encoder](#audio-encoder)\n",
    "   - [Visual Encoder](#visual-encoder)\n",
    "   - [Cross-Modal Fusion](#cross-modal-fusion)\n",
    "3. [Training Pipeline](#training-pipeline)\n",
    "4. [Inference and Evaluation](#inference)\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "### Image Preprocessing\n",
    "\n",
    "The image preprocessing pipeline includes several key steps:\n",
    "\n",
    "1. **Frame Extraction**: Extract frames from video at uniform intervals\n",
    "2. **Resizing**: Resize frames to a standard size (224x224)\n",
    "3. **Normalization**: Normalize pixel values using ImageNet statistics\n",
    "\n",
    "#### Normalization Explained\n",
    "\n",
    "The normalization step uses ImageNet statistics:\n",
    "```python\n",
    "transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],  # RGB means\n",
    "    std=[0.229, 0.224, 0.225]    # RGB standard deviations\n",
    ")\n",
    "```\n",
    "\n",
    "Why these specific values?\n",
    "- These are the mean and standard deviation of the ImageNet dataset\n",
    "- Used in pre-training of many vision models (ResNet, VGG, ViT)\n",
    "- Helps with:\n",
    "  - Numerical stability\n",
    "  - Faster convergence\n",
    "  - Better generalization\n",
    "\n",
    "#### Code Example\n",
    "```python\n",
    "def extract_frames(video_path, frame_count=16):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Sample frames uniformly\n",
    "    frame_indices = np.linspace(0, total_frames-1, frame_count, dtype=int)\n",
    "    frames = []\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = transform(frame)\n",
    "            frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return torch.stack(frames)\n",
    "```\n",
    "\n",
    "### Audio Preprocessing\n",
    "\n",
    "The audio preprocessing pipeline includes:\n",
    "\n",
    "1. **Audio Extraction**: Extract audio from video\n",
    "2. **Resampling**: Convert to standard sampling rate (16kHz)\n",
    "3. **Mel Spectrogram**: Convert to mel spectrogram representation\n",
    "\n",
    "#### Mel Spectrograms Explained\n",
    "\n",
    "A mel spectrogram is a visual representation of the short-term power spectrum of a sound, using a mel scale. Here's why and how we use them:\n",
    "\n",
    "1. **What is a Mel Scale?**\n",
    "   - A non-linear scale of pitches\n",
    "   - Based on human perception of pitch\n",
    "   - More sensitive to changes in lower frequencies\n",
    "   - Less sensitive to changes in higher frequencies\n",
    "\n",
    "2. **Why Use Mel Spectrograms?**\n",
    "   - Better represents how humans perceive sound\n",
    "   - Captures important speech characteristics\n",
    "   - Reduces dimensionality while preserving important features\n",
    "   - Standard input format for many audio models\n",
    "\n",
    "3. **How They're Created**\n",
    "   ```python\n",
    "   mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "       sample_rate=16000,    # Audio sampling rate\n",
    "       n_fft=1024,          # Size of FFT window\n",
    "       hop_length=512,       # Number of samples between successive frames\n",
    "       n_mels=64            # Number of mel bands\n",
    "   )(waveform)\n",
    "   ```\n",
    "\n",
    "   The process involves:\n",
    "   1. Short-time Fourier transform (STFT)\n",
    "   2. Convert to mel scale using triangular filters\n",
    "   3. Convert to log scale\n",
    "\n",
    "4. **Visualization Example**\n",
    "   ```python\n",
    "   def plot_mel_spectrogram(mel_spec):\n",
    "       plt.figure(figsize=(10, 4))\n",
    "       plt.imshow(mel_spec.squeeze().numpy(), \n",
    "                  aspect='auto', \n",
    "                  origin='lower')\n",
    "       plt.colorbar(format='%+2.0f dB')\n",
    "       plt.title('Mel Spectrogram')\n",
    "       plt.xlabel('Time')\n",
    "       plt.ylabel('Mel Frequency')\n",
    "       plt.show()\n",
    "   ```\n",
    "\n",
    "#### Resampling Explained\n",
    "\n",
    "Why resample to 16kHz?\n",
    "- Different audio files have different sampling rates\n",
    "- 16kHz is standard for speech processing\n",
    "- Sufficient for capturing speech frequencies (300Hz-3kHz)\n",
    "- Memory efficient while maintaining quality\n",
    "\n",
    "#### Code Example\n",
    "```python\n",
    "def extract_audio(video_path, sample_rate=16000, duration=1.0):\n",
    "    # Extract audio using ffmpeg\n",
    "    !ffmpeg -y -i {video_path} -vn -ar {sample_rate} -ac 1 -f wav temp_audio.wav\n",
    "    \n",
    "    # Load audio and compute mel spectrogram\n",
    "    waveform, sr = torchaudio.load('temp_audio.wav')\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Extract segment of specified length\n",
    "    target_length = int(duration * sample_rate)\n",
    "    if waveform.shape[1] > target_length:\n",
    "        start = torch.randint(0, waveform.shape[1] - target_length, (1,))\n",
    "        waveform = waveform[:, start:start + target_length]\n",
    "    else:\n",
    "        pad_length = target_length - waveform.shape[1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, pad_length))\n",
    "    \n",
    "    # Compute mel spectrogram\n",
    "    mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    "    )(waveform)\n",
    "    \n",
    "    return mel_spec\n",
    "```\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "The AVFF model consists of several key components:\n",
    "\n",
    "1. **Audio Encoder**: Processes audio features\n",
    "2. **Visual Encoder**: Processes visual features\n",
    "3. **Cross-Modal Fusion**: Combines audio and visual features\n",
    "4. **Decoder**: Reconstructs input features\n",
    "5. **Classifier**: Makes final deepfake prediction\n",
    "\n",
    "### Audio Encoder\n",
    "\n",
    "Uses Wav2Vec2 pre-trained model for audio feature extraction:\n",
    "```python\n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained('facebook/wav2vec2-base')\n",
    "        self.projection = nn.Linear(768, 512)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x).last_hidden_state\n",
    "        features = torch.mean(features, dim=1)\n",
    "        return self.projection(features)\n",
    "```\n",
    "\n",
    "### Visual Encoder\n",
    "\n",
    "Uses ViT (Vision Transformer) for visual feature extraction:\n",
    "```python\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        self.projection = nn.Linear(768, 512)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, frames = x.shape[:2]\n",
    "        x = x.view(-1, *x.shape[2:])\n",
    "        features = self.encoder(x)\n",
    "        features = features.view(batch_size, frames, -1)\n",
    "        features = torch.mean(features, dim=1)\n",
    "        return self.projection(features)\n",
    "```\n",
    "\n",
    "### Cross-Modal Fusion\n",
    "\n",
    "Combines audio and visual features using attention:\n",
    "```python\n",
    "class CrossModalFusion(nn.Module):\n",
    "    def __init__(self, dim=512):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(dim, num_heads=8)\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, visual_features, audio_features):\n",
    "        attn_out, _ = self.attention(\n",
    "            visual_features.unsqueeze(0),\n",
    "            audio_features.unsqueeze(0),\n",
    "            audio_features.unsqueeze(0)\n",
    "        )\n",
    "        fused = self.fusion_mlp(torch.cat([visual_features, attn_out.squeeze(0)], dim=1))\n",
    "        return fused\n",
    "```\n",
    "\n",
    "## Training Pipeline\n",
    "\n",
    "The training process consists of two stages:\n",
    "\n",
    "1. **Self-Supervised Learning**:\n",
    "   - Train encoder-decoder architecture\n",
    "   - Reconstruct input features\n",
    "   - Learn meaningful representations\n",
    "\n",
    "2. **Supervised Learning**:\n",
    "   - Fine-tune for deepfake detection\n",
    "   - Use labeled data\n",
    "   - Optimize for classification\n",
    "\n",
    "## Inference and Evaluation\n",
    "\n",
    "The inference pipeline:\n",
    "1. Preprocess input video\n",
    "2. Extract features using encoders\n",
    "3. Fuse features\n",
    "4. Make prediction\n",
    "\n",
    "```python\n",
    "def process_video(video_path):\n",
    "    # Extract frames and audio\n",
    "    frames = extract_frames(video_path)\n",
    "    mel_spec = extract_audio(video_path)\n",
    "    \n",
    "    # Move to device\n",
    "    frames = frames.unsqueeze(0).to(device)\n",
    "    mel_spec = mel_spec.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get features\n",
    "    with torch.no_grad():\n",
    "        audio_features = audio_encoder(mel_spec)\n",
    "        visual_features = visual_encoder(frames)\n",
    "        fused_features = fusion(visual_features, audio_features)\n",
    "        prediction = classifier(fused_features)\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction.item(),\n",
    "        'audio_features': audio_features.cpu().numpy(),\n",
    "        'visual_features': visual_features.cpu().numpy(),\n",
    "        'fused_features': fused_features.cpu().numpy()\n",
    "    }\n",
    "```\n",
    "\n",
    "## References\n",
    "\n",
    "1. AVFF Paper (CVPR 2024)\n",
    "2. Wav2Vec2: [facebook/wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base)\n",
    "3. ViT: [timm/vit_base_patch16_224](https://github.com/huggingface/pytorch-image-models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}