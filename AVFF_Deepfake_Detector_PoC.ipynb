{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç AVFF Deepfake Detection - End-to-End PoC Notebook\n",
    "Based on: CVPR 2024 Paper [AVFF - Audio-Visual Feature Fusion](https://arxiv.org/abs/2403.09387)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üîß Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio librosa opencv-python matplotlib
",
    "!pip install transformers einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üì• Load & Preprocess Sample Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, os
",
    "import librosa
",
    "import numpy as np
",
    "import matplotlib.pyplot as plt
",
    "import torchaudio
",
    "from pathlib import Path
",
    "
",
    "# Helper: extract frames
",
    "def extract_frames(video_path, output_dir, frame_rate=5):
",
    "    os.makedirs(output_dir, exist_ok=True)
",
    "    cap = cv2.VideoCapture(video_path)
",
    "    fps = cap.get(cv2.CAP_PROP_FPS)
",
    "    interval = int(fps // frame_rate)
",
    "    count, saved = 0, 0
",
    "    while cap.isOpened():
",
    "        ret, frame = cap.read()
",
    "        if not ret:
",
    "            break
",
    "        if count % interval == 0:
",
    "            cv2.imwrite(f"{output_dir}/frame_{saved:03d}.jpg", frame)
",
    "            saved += 1
",
    "        count += 1
",
    "    cap.release()
",
    "    return saved
",
    "
",
    "# Helper: extract audio and compute spectrogram
",
    "def get_mel_spectrogram(audio_path, sr=16000):
",
    "    waveform, sample_rate = torchaudio.load(audio_path)
",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=sr)(waveform)
",
    "    mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_mels=64)(waveform)
",
    "    return mel_spec.squeeze(0).numpy()
",
    "
",
    "# Test paths (you can replace with your own)
",
    "video_path = 'sample.mp4'  # add a test video
",
    "output_dir = 'processed_frames'
",
    "extract_frames(video_path, output_dir)
",
    "!ffmpeg -y -i sample.mp4 -vn -ar 16000 -ac 1 -f wav sample.wav
",
    "mel = get_mel_spectrogram('sample.wav')
",
    "plt.imshow(mel, aspect='auto', origin='lower'); plt.title('Mel Spectrogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üéõÔ∏è Build Audio & Visual Encoders (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch
",
    "import torch.nn as nn
",
    "from einops import rearrange
",
    "
",
    "class SimpleTransformerEncoder(nn.Module):
",
    "    def __init__(self, input_dim, hidden_dim=256, depth=4, heads=4):
",
    "        super().__init__()
",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=input_dim, nhead=heads, dim_feedforward=hidden_dim)
",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)
",
    "    def forward(self, x):
",
    "        return self.encoder(x)
",
    "
",
    "class AudioEncoder(nn.Module):
",
    "    def __init__(self, n_mels=64):
",
    "        super().__init__()
",
    "        self.patch_embed = nn.Linear(n_mels, 128)
",
    "        self.encoder = SimpleTransformerEncoder(input_dim=128)
",
    "    def forward(self, mel):
",
    "        x = torch.tensor(mel).T.unsqueeze(0).float()  # [1, time, n_mels]
",
    "        x = self.patch_embed(x)
",
    "        return self.encoder(x)
",
    "
",
    "# Test
",
    "audio_encoder = AudioEncoder()
",
    "audio_features = audio_encoder(mel)
",
    "print("Audio features:", audio_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üîÅ Cross-Modal A2V / V2A Fusion (Simplified Mock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate complementary masking
",
    "def complementary_mask(tensor, mask_ratio=0.5):
",
    "    total = tensor.shape[1]
",
    "    mask = torch.randperm(total)
",
    "    half = total // 2
",
    "    audio_keep = mask[:half]
",
    "    video_keep = mask[half:]
",
    "    return audio_keep, video_keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üß† Deepfake Classification (Toy Example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepfakeClassifier(nn.Module):
",
    "    def __init__(self, input_dim=128):
",
    "        super().__init__()
",
    "        self.classifier = nn.Sequential(
",
    "            nn.Linear(input_dim, 64),
",
    "            nn.ReLU(),
",
    "            nn.Linear(64, 2)
",
    "        )
",
    "    def forward(self, x):
",
    "        return self.classifier(x.mean(dim=1))
",
    "
",
    "# Run classifier
",
    "classifier = DeepfakeClassifier()
",
    "logits = classifier(audio_features)
",
    "print("Logits:", logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}