{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08cf959",
   "metadata": {},
   "source": [
    "# üîç AVFF Deepfake Detection Demo\n",
    "\n",
    "This notebook demonstrates the AVFF (Audio-Visual Feature Fusion) pipeline for video deepfake detection using pre-trained models.\n",
    "\n",
    "## Pipeline Overview\n",
    "1. üîß Setup & Dependencies\n",
    "2. üì• Load & Preprocess Sample Video\n",
    "3. üéõÔ∏è Build Audio & Visual Encoders\n",
    "4. üîÅ Cross-Modal Feature Fusion\n",
    "5. üì¶ Reconstruction via Decoder\n",
    "6. üß† Deepfake Classification\n",
    "7. üß™ Run Inference\n",
    "8. üìà Visualize & Interpret Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396c56bd",
   "metadata": {},
   "source": [
    "## 1. üîß Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb352c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio transformers timm einops librosa opencv-python matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "import torchvision.transforms as transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "from transformers import AutoModel, AutoFeatureExtractor\n",
    "import timm\n",
    "from einops import rearrange\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1752d40",
   "metadata": {},
   "source": [
    "## 2. üì• Load & Preprocess Sample Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a480e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frames(video_path, frame_count=16):\n",
    "    \"\"\"Extract frames from video and preprocess them.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Sample frames uniformly\n",
    "    frame_indices = np.linspace(0, total_frames-1, frame_count, dtype=int)\n",
    "    frames = []\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    for idx in frame_indices:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = transform(frame)\n",
    "            frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    return torch.stack(frames)\n",
    "\n",
    "def extract_audio(video_path, sample_rate=16000, duration=1.0):\n",
    "    \"\"\"Extract audio from video and compute mel spectrogram.\"\"\"\n",
    "    # Extract audio using ffmpeg\n",
    "    !ffmpeg -y -i {video_path} -vn -ar {sample_rate} -ac 1 -f wav temp_audio.wav\n",
    "    \n",
    "    # Load audio and compute mel spectrogram\n",
    "    waveform, sr = torchaudio.load('temp_audio.wav')\n",
    "    if sr != sample_rate:\n",
    "        resampler = torchaudio.transforms.Resample(sr, sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    # Extract segment of specified length\n",
    "    target_length = int(duration * sample_rate)\n",
    "    if waveform.shape[1] > target_length:\n",
    "        start = torch.randint(0, waveform.shape[1] - target_length, (1,))\n",
    "        waveform = waveform[:, start:start + target_length]\n",
    "    else:\n",
    "        pad_length = target_length - waveform.shape[1]\n",
    "        waveform = torch.nn.functional.pad(waveform, (0, pad_length))\n",
    "    \n",
    "    # Compute mel spectrogram\n",
    "    mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sample_rate,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    "    )(waveform)\n",
    "    \n",
    "    return mel_spec\n",
    "\n",
    "# Test with a sample video\n",
    "video_path = 'sample.mp4'  # Replace with your video path\n",
    "frames = extract_frames(video_path)\n",
    "mel_spec = extract_audio(video_path)\n",
    "\n",
    "print(f'Frames shape: {frames.shape}')\n",
    "print(f'Mel spectrogram shape: {mel_spec.shape}')\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(frames[0].permute(1, 2, 0).numpy())\n",
    "plt.title('Sample Frame')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(mel_spec.squeeze().numpy(), aspect='auto', origin='lower')\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6bd8f8",
   "metadata": {},
   "source": [
    "## 3. üéõÔ∏è Build Audio & Visual Encoders\n",
    "\n",
    "We'll use pre-trained models for both audio and visual encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c858f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use Wav2Vec2 for audio encoding\n",
    "        self.encoder = AutoModel.from_pretrained('facebook/wav2vec2-base')\n",
    "        self.projection = nn.Linear(768, 512)  # Project to common dimension\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, 1, time]\n",
    "        features = self.encoder(x).last_hidden_state\n",
    "        # Pool over time dimension\n",
    "        features = torch.mean(features, dim=1)\n",
    "        return self.projection(features)\n",
    "\n",
    "class VisualEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use ViT for visual encoding\n",
    "        self.encoder = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
    "        self.projection = nn.Linear(768, 512)  # Project to common dimension\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, frames, channels, height, width]\n",
    "        batch_size, frames = x.shape[:2]\n",
    "        x = x.view(-1, *x.shape[2:])  # Combine batch and frame dimensions\n",
    "        features = self.encoder(x)\n",
    "        features = features.view(batch_size, frames, -1)\n",
    "        # Pool over frames\n",
    "        features = torch.mean(features, dim=1)\n",
    "        return self.projection(features)\n",
    "\n",
    "# Initialize encoders\n",
    "audio_encoder = AudioEncoder().to(device)\n",
    "visual_encoder = VisualEncoder().to(device)\n",
    "\n",
    "# Test encoders\n",
    "with torch.no_grad():\n",
    "    audio_features = audio_encoder(mel_spec.unsqueeze(0).to(device))\n",
    "    visual_features = visual_encoder(frames.unsqueeze(0).to(device))\n",
    "    \n",
    "print(f'Audio features shape: {audio_features.shape}')\n",
    "print(f'Visual features shape: {visual_features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ab70e",
   "metadata": {},
   "source": [
    "## 4. üîÅ Cross-Modal Feature Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc768ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossModalFusion(nn.Module):\n",
    "    def __init__(self, dim=512):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(dim, num_heads=8)\n",
    "        self.fusion_mlp = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, visual_features, audio_features):\n",
    "        # Cross-attention\n",
    "        attn_out, _ = self.attention(\n",
    "            visual_features.unsqueeze(0),\n",
    "            audio_features.unsqueeze(0),\n",
    "            audio_features.unsqueeze(0)\n",
    "        )\n",
    "        \n",
    "        # Fusion\n",
    "        fused = self.fusion_mlp(torch.cat([visual_features, attn_out.squeeze(0)], dim=1))\n",
    "        return fused\n",
    "\n",
    "# Initialize fusion module\n",
    "fusion = CrossModalFusion().to(device)\n",
    "\n",
    "# Test fusion\n",
    "with torch.no_grad():\n",
    "    fused_features = fusion(visual_features, audio_features)\n",
    "print(f'Fused features shape: {fused_features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7724ac",
   "metadata": {},
   "source": [
    "## 5. üì¶ Reconstruction via Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefba65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim * 2, 64 * 64)  # Reconstruct mel spectrogram\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "# Initialize decoder\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "# Test reconstruction\n",
    "with torch.no_grad():\n",
    "    reconstructed = decoder(fused_features)\n",
    "    reconstructed = reconstructed.view(-1, 64, 64)\n",
    "\n",
    "# Visualize reconstruction\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(mel_spec.squeeze().numpy(), aspect='auto', origin='lower')\n",
    "plt.title('Original Mel Spectrogram')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(reconstructed.squeeze().cpu().numpy(), aspect='auto', origin='lower')\n",
    "plt.title('Reconstructed Mel Spectrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c27ae2",
   "metadata": {},
   "source": [
    "## 6. üß† Deepfake Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b5a9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim=512):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "\n",
    "# Initialize classifier\n",
    "classifier = Classifier().to(device)\n",
    "\n",
    "# Test classification\n",
    "with torch.no_grad():\n",
    "    prediction = classifier(fused_features)\n",
    "print(f'Prediction: {prediction.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0da13b",
   "metadata": {},
   "source": [
    "## 7. üß™ Run Inference on a Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91360e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(video_path):\n",
    "    # Extract frames and audio\n",
    "    frames = extract_frames(video_path)\n",
    "    mel_spec = extract_audio(video_path)\n",
    "    \n",
    "    # Move to device\n",
    "    frames = frames.unsqueeze(0).to(device)\n",
    "    mel_spec = mel_spec.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get features\n",
    "    with torch.no_grad():\n",
    "        audio_features = audio_encoder(mel_spec)\n",
    "        visual_features = visual_encoder(frames)\n",
    "        fused_features = fusion(visual_features, audio_features)\n",
    "        prediction = classifier(fused_features)\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction.item(),\n",
    "        'audio_features': audio_features.cpu().numpy(),\n",
    "        'visual_features': visual_features.cpu().numpy(),\n",
    "        'fused_features': fused_features.cpu().numpy()\n",
    "    }\n",
    "\n",
    "# Test on a video\n",
    "video_path = 'sample.mp4'  # Replace with your video path\n",
    "results = process_video(video_path)\n",
    "print(f'Deepfake probability: {results[\"prediction\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8249c14",
   "metadata": {},
   "source": [
    "## 8. üìà Visualize & Interpret Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8cb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(results):\n",
    "    # Plot feature distributions\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(results['audio_features'].flatten(), bins=50)\n",
    "    plt.title('Audio Features Distribution')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(results['visual_features'].flatten(), bins=50)\n",
    "    plt.title('Visual Features Distribution')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(results['fused_features'].flatten(), bins=50)\n",
    "    plt.title('Fused Features Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize features\n",
    "visualize_features(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
